

import pandas as pd
import numpy as np
import time
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, hamming_loss, f1_score, precision_score, recall_score, jaccard_score
from sklearn.preprocessing import StandardScaler
from skmultilearn.problem_transform import ClassifierChain
from xgboost import XGBClassifier

df = pd.read_csv(r"C:\Users\HP\Dropbox\PHD\level 2\SWE620\project\Fdroid\class_multi_label_balanced_ml_ros.csv")

X = df.iloc[:, 1:-7]
y = df.iloc[:, -7:]

# ▶️ Step 3: Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ▶️ Step 4: 10-Fold Cross-Validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)

subset_accuracies = []
jaccard_accuracies = []
hamming_losses = []
f1_macros = []
f1_micros = []
precision_macros = []
precision_micros = []
recall_macros = []
recall_micros = []

# Start timing the entire process
start_time = time.time()

fold = 1
for train_index, test_index in kf.split(X_scaled):
    print(f"Training Fold {fold}...")

    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    xgb_classifier =   XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            use_label_encoder=False,
            learning_rate=0.1,
            max_depth=10,
            n_estimators=500,
            n_jobs=-1,
            random_state=42
        )
    #XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

    # Wrap it in a Classifier Chain
    cc_classifier = ClassifierChain(classifier=xgb_classifier)

    # Fit model on training data
    cc_classifier.fit(X_train, y_train)

    # Predict
    y_pred = cc_classifier.predict(X_test)

    # Evaluation Metrics
    subset_acc = accuracy_score(y_test, y_pred)
    jaccard_acc = jaccard_score(y_test, y_pred, average='samples')
    ham_loss = hamming_loss(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_micro = f1_score(y_test, y_pred, average='micro')
    precision_macro = precision_score(y_test, y_pred, average='macro')
    precision_micro = precision_score(y_test, y_pred, average='micro')
    recall_macro = recall_score(y_test, y_pred, average='macro')
    recall_micro = recall_score(y_test, y_pred, average='micro')

    # Store metrics
    subset_accuracies.append(subset_acc)
    jaccard_accuracies.append(jaccard_acc)
    hamming_losses.append(ham_loss)
    f1_macros.append(f1_macro)
    f1_micros.append(f1_micro)
    precision_macros.append(precision_macro)
    precision_micros.append(precision_micro)
    recall_macros.append(recall_macro)
    recall_micros.append(recall_micro)

    print(f"Fold {fold} Completed.")
    fold += 1

# End timing
end_time = time.time()
training_duration = (end_time - start_time) / 60  # in minutes

# ▶️ Step 5: Display Cross-Validation Results
print("\n--- 10-Fold Cross-Validation Results (Classifier Chain + XGBoost) ---")
print(f"Subset Accuracy    : {np.mean(subset_accuracies):.4f} (+/- {np.std(subset_accuracies):.4f})")
print(f"Jaccard Accuracy    : {np.mean(jaccard_accuracies):.4f} (+/- {np.std(jaccard_accuracies):.4f})")
print(f"Hamming Loss       : {np.mean(hamming_losses):.4f} (+/- {np.std(hamming_losses):.4f})")
print(f"F1 Macro           : {np.mean(f1_macros):.4f} (+/- {np.std(f1_macros):.4f})")
print(f"F1 Micro           : {np.mean(f1_micros):.4f} (+/- {np.std(f1_micros):.4f})")
print(f"Precision Macro    : {np.mean(precision_macros):.4f} (+/- {np.std(precision_macros):.4f})")
print(f"Precision Micro    : {np.mean(precision_micros):.4f} (+/- {np.std(precision_micros):.4f})")
print(f"Recall Macro       : {np.mean(recall_macros):.4f} (+/- {np.std(recall_macros):.4f})")
print(f"Recall Micro       : {np.mean(recall_micros):.4f} (+/- {np.std(recall_micros):.4f})")
print(f"\nTotal Training Time: {training_duration:.2f} minutes")
