import pandas as pd
import numpy as np
import time
from xgboost import XGBClassifier
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.metrics import (
    accuracy_score,
    hamming_loss,
    f1_score,
    precision_score,
    recall_score,
    jaccard_score
)
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

# === Step 1: Load the balanced multi-label dataset ===
file_path = r"C:\Users\HP\Dropbox\PHD\level 2\SWE620\project\Fdroid\method\method_multi_label_balanced_ml_ros.csv"
df = pd.read_csv(file_path)

# === Step 2: Define feature columns and label columns ===
label_columns = [
    'Rename Method',
    'Extract Method',
    'Inline Method',
    'Move Method',
    'Extract And Move Method',
    'Push Down Method',
    'Pull Up Method'
]

feature_columns = [col for col in df.columns if col not in label_columns + ['className', 'fullMethodName']]

# === Step 3: Prepare features and labels ===
X = df[feature_columns].values
y = df[label_columns].values

# === Step 4: Set up 10-fold cross-validation ===
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Lists to store results for each fold
hamming_losses = []
subset_accuracies = []
f1_macros = []
f1_micros = []
precision_macros = []
precision_micros = []
recall_macros = []
recall_micros = []
jaccard_macros = []
jaccard_micros = []
jaccard_means = []

# === Step 5: Perform 10-fold cross-validation ===
start_time = time.time()
fold = 1
for train_index, test_index in kf.split(X):
    print(f"\n=== Fold {fold} ===")

    # Split the data
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Create Binary Relevance classifier with XGBoost
    br_classifier = BinaryRelevance(
        classifier=XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            use_label_encoder=False,
            learning_rate=0.1,
            max_depth=10,
            n_estimators=500,
            n_jobs=-1,
            random_state=42
        )
    )

    # Train the model
    br_classifier.fit(X_train, y_train)

    # Predict
    y_pred = br_classifier.predict(X_test)

    # Convert to dense if sparse
    if hasattr(y_pred, "toarray"):
        y_pred = y_pred.toarray()

    # === Metrics Calculation ===
    hamming = hamming_loss(y_test, y_pred)
    subset_acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
    f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    precision_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    recall_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)
    jaccard_macro = jaccard_score(y_test, y_pred, average='macro', zero_division=0)
    jaccard_micro = jaccard_score(y_test, y_pred, average='micro', zero_division=0)
    jaccard_mean = jaccard_score(y_test, y_pred, average='samples', zero_division=0)

    # === Append metrics ===
    hamming_losses.append(hamming)
    subset_accuracies.append(subset_acc)
    f1_macros.append(f1_macro)
    f1_micros.append(f1_micro)
    precision_macros.append(precision_macro)
    precision_micros.append(precision_micro)
    recall_macros.append(recall_macro)
    recall_micros.append(recall_micro)
    jaccard_macros.append(jaccard_macro)
    jaccard_micros.append(jaccard_micro)
    jaccard_means.append(jaccard_mean)

    # === Print results for this fold ===
    print(f"Hamming Loss        : {hamming:.4f}")
    print(f"Subset Accuracy     : {subset_acc:.4f}")
    print(f"Jaccard Macro       : {jaccard_macro:.4f}")
    print(f"Jaccard Micro       : {jaccard_micro:.4f}")
    print(f"Jaccard Mean        : {jaccard_mean:.4f}")
    print(f"F1 Macro            : {f1_macro:.4f}")
    print(f"F1 Micro            : {f1_micro:.4f}")
    print(f"Precision Macro     : {precision_macro:.4f}")
    print(f"Precision Micro     : {precision_micro:.4f}")
    print(f"Recall Macro        : {recall_macro:.4f}")
    print(f"Recall Micro        : {recall_micro:.4f}")

    fold += 1

end_time = time.time()
total_training_time = (end_time - start_time) / 60  # in minutes

# === Step 6: Report overall cross-validation results (formatted) ===
print("\n===========================================")
print("10 Fold Cross Validation Results (BR + XGB)")
print("===========================================\n")

print(f"Subset Accuracy   : {np.mean(subset_accuracies):.4f} ± {np.std(subset_accuracies):.4f}")
print(f"Jaccard Mean      : {np.mean(jaccard_means):.4f} ± {np.std(jaccard_means):.4f}")
print(f"Hamming Loss      : {np.mean(hamming_losses):.4f} ± {np.std(hamming_losses):.4f}")
print(f"F1 Macro          : {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}")
print(f"F1 Micro          : {np.mean(f1_micros):.4f} ± {np.std(f1_micros):.4f}")
print(f"Precision Macro   : {np.mean(precision_macros):.4f} ± {np.std(precision_macros):.4f}")
print(f"Precision Micro   : {np.mean(precision_micros):.4f} ± {np.std(precision_micros):.4f}")
print(f"Recall Macro      : {np.mean(recall_macros):.4f} ± {np.std(recall_macros):.4f}")
print(f"Recall Micro      : {np.mean(recall_micros):.4f} ± {np.std(recall_micros):.4f}")
print(f"\nTotal Training Time: {total_training_time:.2f} minutes")

# === Optional Step 7: Train on all data and plot label distribution in predictions ===
final_br_classifier = BinaryRelevance(
    classifier=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
)

final_br_classifier.fit(X, y)
y_pred_all = final_br_classifier.predict(X)

if hasattr(y_pred_all, "toarray"):
    y_pred_all = y_pred_all.toarray()

# Label distribution in predictions
pred_label_counts = pd.DataFrame(y_pred_all, columns=label_columns).sum()

plt.figure(figsize=(10, 6))
plt.bar(pred_label_counts.index, pred_label_counts.values, color='darkred')
plt.title("Label Distribution in Predictions (BR + XGB, Full Data)")
plt.xlabel("Refactoring Labels")
plt.ylabel("Frequency")
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()
